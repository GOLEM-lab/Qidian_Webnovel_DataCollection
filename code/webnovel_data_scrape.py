# -*- coding: utf-8 -*-
"""webnovel_data_scrape.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bmh6p5bbltQw4dZ16-WIgvZaDCWxpZ05

This is the scrap the metadata of the story on webnovel.com
"""

import os
import re
import json
import time
import requests
import numpy as np
import pandas as pd
from lxml import html
from tqdm import tqdm
from bs4 import BeautifulSoup as bs

headers = {
        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
    }



def get_userInfo(userId):
  user_url = 'https://www.webnovel.com/profile/' + str(userId)
  response = requests.get(user_url,headers=headers)
  soup = bs(response.text,'html')
  user_stats = soup.find_all('ul',{'class':'stat'})[0]
  user_stats = user_stats.text.replace(' ','')
  user_details = soup.find_all('section',{'class':'pb32'})[0]
  wd = re.search(r"([0-9]+)d",user_stats)
  rh = re.search(r"([0-9,\.]+)h",user_stats)
  rb = re.search(r"([0-9]+)R",user_stats)
  name = user_details.find_all('h3')[0].text

  if wd:
    writingDays = wd.group(1)
  else:
    writingDays = None
  if rh:
    readingHours = rh.group(1)
  else:
    readingHours = None
  if rb:
    readBooks = rb.group(1)
  else:
    readBooks = None

  if user_details.find_all('i'):
    gender = user_details.find_all('i')[0].text
  else:
    gender = None

  level = user_details.find_all('strong',{'class':"g_lv"})[0].text.split('LV ')[1]

  if user_details.find_all('p'):
    blurb = user_details.find_all('p')[0].text
  else:
    blurb = None

  if user_details.find_all('address'):
    text = user_details.find_all('address')[0].text
    if 'Joined' in text:
      dateJoined = text.split('Joined')[0].strip()
      location = text.split('Joined')[1].strip()
    else:
      dateJoined = '1970-01-01'
      location = text.strip()
  else:
    dateJoined = '1970-01-01'
    location = 'Unknown'


  user_info = {'userId':userId,'Name':name,'Gender':gender,'Level':level,'writingDays':writingDays,'readingHours':readingHours,
               'readBooks':readBooks,'Description':blurb,'dateJoined':dateJoined,'Location':location}

  return user_info

def main():
	with open('userIds.txt','r') as fout:
		userIds = fout.readlines()
	all_user_info = []
	uniqueUserIds = list(set(userIds))
	for uid in uniqueUserIds[:10000]:
	  all_user_info.append(get_userInfo(uid))
	user_df = pd.DataFrame(all_user_info)
	user_df.to_csv('webnovelUserProfiles.csv',index=False)




if __name__ == "__main__":
    main() 

